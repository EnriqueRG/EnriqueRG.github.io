---
title: "Replicating Wigner's Insight"
date: 2026-02-14
permalink: /posts/post1/
tags:
  - Random Matrix Theory
  - Symbolic Regression
  - Experimental Mathematics
---


Headings are cool
======

I was introduced to the idea of experimental mathematics by my friend Grigory Sarnitsky. 


The story goes that during the conference on Neutron Physics by Time-of-Flight, held at Gatlinburg, Tennessee, November 1 and 2, 1956, Wigner delivered a presentation on the theoretical arrangement of neighboring neutron resonances (with matching spin and parity) in heavy nuclei. In the presentation he gave the following guess:

"Perhaps I am now too courageous when I try to guess the distribution of the distances between successive levels (of energies of heavy nuclei). Theoretically, the situation is quite simple if one attacks the problem in a simpleminded fashion. The question is simply what are the distances of the characteristic values of a symmetric matrix with random coefficients."

At the time of statement, there was little data to prove the surmise. The situation quickly improved as people gathered more data. Firk et al. began a project in 1956, finishing in 1960, studying the first 100 resonances in 238U + n at energies up to 8keV. The data ruled out an exponential distribution and provided the best then-available evidence in support of Wigner's surmise.

Numerically, the question is even simpler, and it is one of the basic result from Random Matrix Theory. The eigenvalue spacing for large real symmetric matrices with random entries is given by 
$$
p_w(s) = \frac{\pi s}{2} e^{-\frac{\pi}{4} s^2},
$$
where $s=S/D$ for a particular spacing ($S$) and  mean distance between neighboring intervals ($D$). The use of $s$ as a variable is called "spectral unfolding" and it is meant to preserve a finite limit as the size of the matrices get large and the eigenvalue spacing naturally tends to zero. The unfolding is such that the mean eigenvalue spacing is unity.

In this notebook, I want to reproduce the "guess" of Wigner using symbolic regression.

Aren't headings cool?
------